<!-- Generated by SpecStory -->

# 关于输入 Gsheet 或 CSV 文件的询问 (2025-05-19 14:12:02)

_**User**_

你可以接受 gsheet 或者 csv 文件的输入吗？如果可以接受，是否可以给你一个 gsheet 的链接，然后你读取呢？

---

_**Assistant**_



---

是的，我可以接收 CSV 或者 Google Sheet 做为数据源：

1. CSV 文件  
   - 你可以直接把 CSV 文件内容贴到对话里，或者通过工具把 .csv 文件上传，我就能用脚本（如 Python 的 `csv` 模块、Pandas 等）去解析并处理。

2. Google Sheet  
   - 最简单的方法是把表格公开成“任何有链接都可以查看”，然后生成 CSV 导出链接：  
     `https://docs.google.com/spreadsheets/d/【表格ID】/export?format=csv&gid=【工作表GID】`  
   - 把这个链接发给我，我就能通过 HTTP 请求拿到 CSV 数据并解析。  
   - 如果你需要用 Google Sheets API 做更复杂的操作（比如授权访问私有表格、读写权限），则需要：  
     1. 在 GCP 控制台开启 Sheets API，并创建服务账号（获取 `credentials.json`）  
     2. 把服务账号权限授予你的表格  
     3. 我可以拿到 `credentials.json`，用 `gspread` 或官方 Python 客户端来读取/写入表格

你可以先决定用哪种方式，然后把 CSV 文件或导出链接发给我，我这边马上帮你读入并处理。

---

_**User**_

目标表	目标Column	data_source_name	source column	key path
risk.dwd_risk_id_af_vida_di	credit_user_id	VidaData	header_data	header_data.CreditUserId
risk.dwd_risk_id_af_vida_di	request_key	VidaData	request_key	
risk.dwd_risk_id_af_vida_di	data_source_name	VidaData	data_source_name	
risk.dwd_risk_id_af_vida_di	data_source_code	VidaData	data_source_code	
risk.dwd_risk_id_af_vida_di	kyc_event_id	VidaData	resp_raw_data	resp_raw_data.data.kycEventId
risk.dwd_risk_id_af_vida_di	kyc_status_raw	VidaData	resp_raw_data	resp_raw_data.data.kyc_status_raw
risk.dwd_risk_id_af_vida_di	score_pob	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'pob'
risk.dwd_risk_id_af_vida_di	score_full_name	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'full_name'
risk.dwd_risk_id_af_vida_di	score_address	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'address'
risk.dwd_risk_id_af_vida_di	score_province	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'province'
risk.dwd_risk_id_af_vida_di	score_city	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'city'
risk.dwd_risk_id_af_vida_di	score_dob	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'dob'
risk.dwd_risk_id_af_vida_di	score_district	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'district'
risk.dwd_risk_id_af_vida_di	score_village	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'village'
risk.dwd_risk_id_af_vida_di	score_nik	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'nik'
risk.dwd_risk_id_af_vida_di	score_mother_maiden_name	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'mother_maiden_name'
risk.dwd_risk_id_af_vida_di	score_family_card_no	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'family_card_no'
risk.dwd_risk_id_af_vida_di	score_demographic	VidaData	resp_raw_data	resp_raw_data.data.fields  所有score的平均数
risk.dwd_risk_id_af_vida_di	score_selfiephoto	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'selfiePhoto'
risk.dwd_risk_id_af_vida_di	score_liveness	VidaData	resp_raw_data	resp_raw_data.data.fields  field = 'score_liveness'
risk.dwd_risk_id_af_vida_di	trace_id	VidaData	trace_id	
risk.dwd_risk_id_af_vida_di	create_time	VidaData	create_time	
risk.dwd_risk_id_af_vida_di	update_time	VidaData	update_time	
risk.dwd_risk_id_af_vida_di	grass_date	VidaData		

根据这个表格给我生成一个 SparkSQL 第一列是需要 insert 的目标表，第二列是目标表的列名需要你在 insert 的时候 alias 一下，第三列是where的条件 data_source_name = ‘VidaData’ 第四列是用到的 ods 表的列名。ods 表是risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df；key path最后一列是 解析 ods 对应列的 json 路径

---

_**Assistant**_



---

下面是一个完整的 SparkSQL 示例脚本，第一列（隐含在 INSERT INTO）为目标表，第二列为目标表的字段别名，第三列统一为 `data_source_name = 'VidaData'` 的过滤条件，第四列为 ods 表的原始字段，第五列为对应的 JSON 路径（供 `get_json_object` 使用）。  
```sql
-- 将 ODS 表中的 VidaData 数据写入 DWD 表
INSERT INTO TABLE risk.dwd_risk_id_af_vida_di
SELECT
  -- 目标列 credit_user_id，对应 ods.header_data 中的 $.CreditUserId
  get_json_object(header_data, '$.CreditUserId')               AS credit_user_id,
  -- request_key 直接映射
  request_key                                                AS request_key,
  -- data_source_name、data_source_code 直接映射
  data_source_name                                           AS data_source_name,
  data_source_code                                           AS data_source_code,
  -- kyc_event_id / kyc_status_raw
  get_json_object(resp_raw_data, '$.data.kycEventId')          AS kyc_event_id,
  get_json_object(resp_raw_data, '$.data.kyc_status_raw')      AS kyc_status_raw,
  -- 各字段 score_xxx
  get_json_object(resp_raw_data, '$.data.fields.pob')          AS score_pob,
  get_json_object(resp_raw_data, '$.data.fields.full_name')    AS score_full_name,
  get_json_object(resp_raw_data, '$.data.fields.address')      AS score_address,
  get_json_object(resp_raw_data, '$.data.fields.province')     AS score_province,
  get_json_object(resp_raw_data, '$.data.fields.city')         AS score_city,
  get_json_object(resp_raw_data, '$.data.fields.dob')          AS score_dob,
  get_json_object(resp_raw_data, '$.data.fields.district')     AS score_district,
  get_json_object(resp_raw_data, '$.data.fields.village')      AS score_village,
  get_json_object(resp_raw_data, '$.data.fields.nik')          AS score_nik,
  get_json_object(resp_raw_data, '$.data.fields.mother_maiden_name')  AS score_mother_maiden_name,
  get_json_object(resp_raw_data, '$.data.fields.family_card_no')       AS score_family_card_no,
  -- 所有 score_xxx 的平均值
  (
    cast(get_json_object(resp_raw_data, '$.data.fields.pob')           AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.full_name')     AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.address')       AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.province')      AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.city')          AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.dob')           AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.district')      AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.village')       AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.nik')           AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.mother_maiden_name') AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.family_card_no')    AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.selfiePhoto')      AS double) +
    cast(get_json_object(resp_raw_data, '$.data.fields.score_liveness')   AS double)
  ) / 13                                                        AS score_demographic,
  get_json_object(resp_raw_data, '$.data.fields.selfiePhoto')  AS score_selfiephoto,
  get_json_object(resp_raw_data, '$.data.fields.score_liveness') AS score_liveness,
  -- trace_id、create_time、update_time 直接映射
  trace_id                                                   AS trace_id,
  create_time                                                AS create_time,
  update_time                                                AS update_time,
  -- grass_date 使用当前日期作为分区字段
  current_date()                                             AS grass_date
FROM
  risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df
WHERE
  data_source_name = 'VidaData'
;
```  
说明：  
- ods 表：`risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df`  
- 过滤条件统一为 `data_source_name = 'VidaData'`  
- JSON 解析均使用 `get_json_object(原始列, 'JSON 路径')`  
- `score_demographic` 为所有 13 个 score 字段求平均值（cast 为 double 后相加再除以 13）  
- 最后一列 `grass_date` 采用 `current_date()` 填充分区日期。

---

_**User**_

其中对于 score_pob 到 score_liveness 的解析有点问题，可能是我表达的也有一些问题，对于 get_json_object(resp_raw_data, '$.data.fields') 解析出来的 json 长这样：[
  {
    "score": 10.0,
    "field": "pob"
  },
  {
    "score": 10.0,
    "field": "full_name"
  },
  {
    "score": 10.0,
    "field": "address"
  },
  {
    "score": 10.0,
    "field": "province"
  },
  {
    "score": 10.0,
    "field": "city"
  },
  {
    "score": 10.0,
    "field": "dob"
  },
  {
    "score": 10.0,
    "field": "district"
  },
  {
    "score": 10.0,
    "field": "village"
  },
  {
    "score": 10.0,
    "field": "nik"
  },
  {
    "score": 8.47,
    "field": "selfiePhoto"
  },
  {
    "score": 0.886551,
    "field": "liveness"
  }
]
我是希望解析得到field 对应的 字段所对应的 score 


---

_**Assistant**_

明白了，对于这种数组格式的 JSON，我们需要使用 `get_json_object` 配合 `json_tuple` 或者 `lateral view explode` 来解析。以下是修改后的 SQL：

```sql
-- 将 ODS 表中的 VidaData 数据写入 DWD 表
INSERT INTO TABLE risk.dwd_risk_id_af_vida_di
WITH parsed_fields AS (
  SELECT 
    resp_raw_data,
    header_data,
    request_key,
    data_source_name,
    data_source_code,
    trace_id,
    create_time,
    update_time,
    -- 使用 lateral view explode 展开数组
    field,
    score
  FROM risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df
  LATERAL VIEW explode(
    get_json_object(resp_raw_data, '$.data.fields')
  ) fields AS field, score
  WHERE data_source_name = 'VidaData'
)
SELECT
  get_json_object(header_data, '$.CreditUserId') AS credit_user_id,
  request_key AS request_key,
  data_source_name AS data_source_name,
  data_source_code AS data_source_code,
  get_json_object(resp_raw_data, '$.data.kycEventId') AS kyc_event_id,
  get_json_object(resp_raw_data, '$.data.kyc_status_raw') AS kyc_status_raw,
  -- 使用 CASE WHEN 来匹配每个字段的 score
  MAX(CASE WHEN field = 'pob' THEN score END) AS score_pob,
  MAX(CASE WHEN field = 'full_name' THEN score END) AS score_full_name,
  MAX(CASE WHEN field = 'address' THEN score END) AS score_address,
  MAX(CASE WHEN field = 'province' THEN score END) AS score_province,
  MAX(CASE WHEN field = 'city' THEN score END) AS score_city,
  MAX(CASE WHEN field = 'dob' THEN score END) AS score_dob,
  MAX(CASE WHEN field = 'district' THEN score END) AS score_district,
  MAX(CASE WHEN field = 'village' THEN score END) AS score_village,
  MAX(CASE WHEN field = 'nik' THEN score END) AS score_nik,
  MAX(CASE WHEN field = 'mother_maiden_name' THEN score END) AS score_mother_maiden_name,
  MAX(CASE WHEN field = 'family_card_no' THEN score END) AS score_family_card_no,
  -- 计算所有 score 的平均值
  AVG(score) AS score_demographic,
  MAX(CASE WHEN field = 'selfiePhoto' THEN score END) AS score_selfiephoto,
  MAX(CASE WHEN field = 'liveness' THEN score END) AS score_liveness,
  trace_id AS trace_id,
  create_time AS create_time,
  update_time AS update_time,
  current_date() AS grass_date
FROM parsed_fields
GROUP BY 
  resp_raw_data,
  header_data,
  request_key,
  data_source_name,
  data_source_code,
  trace_id,
  create_time,
  update_time
;
```

主要修改说明：
1. 使用 `WITH` 子句创建 CTE，通过 `LATERAL VIEW explode` 展开 JSON 数组
2. 使用 `CASE WHEN` 语句来匹配每个字段对应的 score 值
3. 使用 `MAX` 聚合函数来获取每个字段的 score（因为展开后会有多行）
4. 使用 `AVG(score)` 来计算所有 score 的平均值
5. 最后通过 `GROUP BY` 来确保每个请求只产生一行数据

这样修改后，就能正确解析出每个 field 对应的 score 值了。比如对于 `pob` 字段，会找到 `field = 'pob'` 的那条记录，然后取其 `score` 值。

---

_**User**_

java.sql.SQLException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.spark.sql.AnalysisException: cannot resolve 'explode(get_json_object(spark_catalog.risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df.resp_raw_data, '$.data.fields'))' due to data type mismatch: input to function explode should be array or map type, not string; line 15 pos 2;

---

_**Assistant**_

啊，明白了。这个错误是因为 `get_json_object` 返回的是字符串，我们需要先用 `from_json` 函数将字符串转换为数组。以下是修改后的 SQL：

```sql
-- 将 ODS 表中的 VidaData 数据写入 DWD 表
INSERT INTO TABLE risk.dwd_risk_id_af_vida_di
WITH parsed_fields AS (
  SELECT 
    resp_raw_data,
    header_data,
    request_key,
    data_source_name,
    data_source_code,
    trace_id,
    create_time,
    update_time,
    -- 使用 lateral view explode 展开数组
    field,
    score
  FROM risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df
  LATERAL VIEW explode(
    from_json(
      get_json_object(resp_raw_data, '$.data.fields'),
      'array<struct<field:string,score:double>>'
    )
  ) fields AS field, score
  WHERE data_source_name = 'VidaData'
)
SELECT
  get_json_object(header_data, '$.CreditUserId') AS credit_user_id,
  request_key AS request_key,
  data_source_name AS data_source_name,
  data_source_code AS data_source_code,
  get_json_object(resp_raw_data, '$.data.kycEventId') AS kyc_event_id,
  get_json_object(resp_raw_data, '$.data.kyc_status_raw') AS kyc_status_raw,
  -- 使用 CASE WHEN 来匹配每个字段的 score
  MAX(CASE WHEN field = 'pob' THEN score END) AS score_pob,
  MAX(CASE WHEN field = 'full_name' THEN score END) AS score_full_name,
  MAX(CASE WHEN field = 'address' THEN score END) AS score_address,
  MAX(CASE WHEN field = 'province' THEN score END) AS score_province,
  MAX(CASE WHEN field = 'city' THEN score END) AS score_city,
  MAX(CASE WHEN field = 'dob' THEN score END) AS score_dob,
  MAX(CASE WHEN field = 'district' THEN score END) AS score_district,
  MAX(CASE WHEN field = 'village' THEN score END) AS score_village,
  MAX(CASE WHEN field = 'nik' THEN score END) AS score_nik,
  MAX(CASE WHEN field = 'mother_maiden_name' THEN score END) AS score_mother_maiden_name,
  MAX(CASE WHEN field = 'family_card_no' THEN score END) AS score_family_card_no,
  -- 计算所有 score 的平均值
  AVG(score) AS score_demographic,
  MAX(CASE WHEN field = 'selfiePhoto' THEN score END) AS score_selfiephoto,
  MAX(CASE WHEN field = 'liveness' THEN score END) AS score_liveness,
  trace_id AS trace_id,
  create_time AS create_time,
  update_time AS update_time,
  current_date() AS grass_date
FROM parsed_fields
GROUP BY 
  resp_raw_data,
  header_data,
  request_key,
  data_source_name,
  data_source_code,
  trace_id,
  create_time,
  update_time
;
```

主要修改：
1. 在 `LATERAL VIEW explode` 之前，使用 `from_json` 函数将 JSON 字符串转换为数组
2. 使用 `array<struct<field:string,score:double>>` 作为 schema 定义，匹配 JSON 数组中的对象结构

这样修改后应该就能正确解析 JSON 数组了。`from_json` 函数会将 JSON 字符串转换为 Spark SQL 可以理解的数组类型，然后 `explode` 就可以正常工作了。

---

_**User**_

java.sql.SQLException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.spark.sql.AnalysisException: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 1 aliases but got field,score 

---

_**Assistant**_

啊，我明白了。在 Spark SQL 中，`LATERAL VIEW explode` 的语法有点不同。我们需要修改 explode 的写法，让它正确展开结构体。以下是修改后的 SQL：

```sql
-- 将 ODS 表中的 VidaData 数据写入 DWD 表
INSERT INTO TABLE risk.dwd_risk_id_af_vida_di
WITH parsed_fields AS (
  SELECT 
    resp_raw_data,
    header_data,
    request_key,
    data_source_name,
    data_source_code,
    trace_id,
    create_time,
    update_time,
    -- 使用 lateral view explode 展开数组
    fields.field AS field,
    fields.score AS score
  FROM risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df
  LATERAL VIEW explode(
    from_json(
      get_json_object(resp_raw_data, '$.data.fields'),
      'array<struct<field:string,score:double>>'
    )
  ) fields
  WHERE data_source_name = 'VidaData'
)
SELECT
  get_json_object(header_data, '$.CreditUserId') AS credit_user_id,
  request_key AS request_key,
  data_source_name AS data_source_name,
  data_source_code AS data_source_code,
  get_json_object(resp_raw_data, '$.data.kycEventId') AS kyc_event_id,
  get_json_object(resp_raw_data, '$.data.kyc_status_raw') AS kyc_status_raw,
  -- 使用 CASE WHEN 来匹配每个字段的 score
  MAX(CASE WHEN field = 'pob' THEN score END) AS score_pob,
  MAX(CASE WHEN field = 'full_name' THEN score END) AS score_full_name,
  MAX(CASE WHEN field = 'address' THEN score END) AS score_address,
  MAX(CASE WHEN field = 'province' THEN score END) AS score_province,
  MAX(CASE WHEN field = 'city' THEN score END) AS score_city,
  MAX(CASE WHEN field = 'dob' THEN score END) AS score_dob,
  MAX(CASE WHEN field = 'district' THEN score END) AS score_district,
  MAX(CASE WHEN field = 'village' THEN score END) AS score_village,
  MAX(CASE WHEN field = 'nik' THEN score END) AS score_nik,
  MAX(CASE WHEN field = 'mother_maiden_name' THEN score END) AS score_mother_maiden_name,
  MAX(CASE WHEN field = 'family_card_no' THEN score END) AS score_family_card_no,
  -- 计算所有 score 的平均值
  AVG(score) AS score_demographic,
  MAX(CASE WHEN field = 'selfiePhoto' THEN score END) AS score_selfiephoto,
  MAX(CASE WHEN field = 'liveness' THEN score END) AS score_liveness,
  trace_id AS trace_id,
  create_time AS create_time,
  update_time AS update_time,
  current_date() AS grass_date
FROM parsed_fields
GROUP BY 
  resp_raw_data,
  header_data,
  request_key,
  data_source_name,
  data_source_code,
  trace_id,
  create_time,
  update_time
;
```

主要修改：
1. 在 `LATERAL VIEW explode` 中，只给一个别名 `fields`
2. 在 SELECT 语句中，通过 `fields.field` 和 `fields.score` 来访问展开后的字段

这样修改后应该就能正确展开 JSON 数组并访问其中的字段了。错误发生的原因是我们之前试图在 `LATERAL VIEW explode` 中直接给两个别名，但实际上应该先给一个别名，然后通过点号访问结构体中的字段。

---

_**User**_

回滚到之前，的代码版本吧，我觉得你可以解析成两个列，然后用 case when 来进行处理

---

_**Assistant**_

好的，我们回到之前的版本，使用 `get_json_object` 直接解析每个字段的 score。以下是修改后的 SQL：

```sql
-- 将 ODS 表中的 VidaData 数据写入 DWD 表
INSERT INTO TABLE risk.dwd_risk_id_af_vida_di
SELECT
  get_json_object(header_data, '$.CreditUserId') AS credit_user_id,
  request_key AS request_key,
  data_source_name AS data_source_name,
  data_source_code AS data_source_code,
  get_json_object(resp_raw_data, '$.data.kycEventId') AS kyc_event_id,
  get_json_object(resp_raw_data, '$.data.kyc_status_raw') AS kyc_status_raw,
  -- 使用 get_json_object 直接解析每个字段的 score
  CAST(get_json_object(resp_raw_data, '$.data.fields[0].score') AS DOUBLE) AS score_pob,
  CAST(get_json_object(resp_raw_data, '$.data.fields[1].score') AS DOUBLE) AS score_full_name,
  CAST(get_json_object(resp_raw_data, '$.data.fields[2].score') AS DOUBLE) AS score_address,
  CAST(get_json_object(resp_raw_data, '$.data.fields[3].score') AS DOUBLE) AS score_province,
  CAST(get_json_object(resp_raw_data, '$.data.fields[4].score') AS DOUBLE) AS score_city,
  CAST(get_json_object(resp_raw_data, '$.data.fields[5].score') AS DOUBLE) AS score_dob,
  CAST(get_json_object(resp_raw_data, '$.data.fields[6].score') AS DOUBLE) AS score_district,
  CAST(get_json_object(resp_raw_data, '$.data.fields[7].score') AS DOUBLE) AS score_village,
  CAST(get_json_object(resp_raw_data, '$.data.fields[8].score') AS DOUBLE) AS score_nik,
  CAST(get_json_object(resp_raw_data, '$.data.fields[9].score') AS DOUBLE) AS score_mother_maiden_name,
  CAST(get_json_object(resp_raw_data, '$.data.fields[10].score') AS DOUBLE) AS score_family_card_no,
  -- 计算所有 score 的平均值
  (
    CAST(get_json_object(resp_raw_data, '$.data.fields[0].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[1].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[2].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[3].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[4].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[5].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[6].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[7].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[8].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[9].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[10].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[11].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[12].score') AS DOUBLE)
  ) / 13 AS score_demographic,
  CAST(get_json_object(resp_raw_data, '$.data.fields[11].score') AS DOUBLE) AS score_selfiephoto,
  CAST(get_json_object(resp_raw_data, '$.data.fields[12].score') AS DOUBLE) AS score_liveness,
  trace_id AS trace_id,
  create_time AS create_time,
  update_time AS update_time,
  current_date() AS grass_date
FROM risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df
WHERE data_source_name = 'VidaData'
;
```

主要修改：
1. 移除了 `WITH` 子句和 `LATERAL VIEW explode`
2. 使用 `get_json_object` 直接通过数组索引访问每个字段的 score
3. 使用 `CAST(...AS DOUBLE)` 确保 score 是数值类型
4. 计算 `score_demographic` 时直接使用所有 score 字段的和除以 13

这个版本应该更简单直接，不需要复杂的数组展开操作。不过需要注意的是，这个方案假设了 fields 数组中字段的顺序是固定的。如果字段顺序可能会变化，我们可能需要先检查每个字段的 `field` 值来确定正确的 score。

---

_**User**_

非常好，你上面的这个 field[x] 没有考虑到乱序的问题，你可以根据这个[
  {
    "score": 10.0,
    "field": "pob"
  },
  {
    "score": 10.0,
    "field": "full_name"
  },
  {
    "score": 10.0,
    "field": "address"
  },
  {
    "score": 10.0,
    "field": "province"
  },
  {
    "score": 10.0,
    "field": "city"
  },
  {
    "score": 10.0,
    "field": "dob"
  },
  {
    "score": 10.0,
    "field": "district"
  },
  {
    "score": 10.0,
    "field": "village"
  },
  {
    "score": 10.0,
    "field": "nik"
  },
  {
    "score": 8.47,
    "field": "selfiePhoto"
  },
  {
    "score": 0.886551,
    "field": "liveness"
  }
]这个 json 格式改一下代码吗

---

_**Assistant**_

好的，我们需要根据 `field` 的值来获取对应的 `score`。以下是修改后的 SQL：

```sql
-- 将 ODS 表中的 VidaData 数据写入 DWD 表
INSERT INTO TABLE risk.dwd_risk_id_af_vida_di
SELECT
  get_json_object(header_data, '$.CreditUserId') AS credit_user_id,
  request_key AS request_key,
  data_source_name AS data_source_name,
  data_source_code AS data_source_code,
  get_json_object(resp_raw_data, '$.data.kycEventId') AS kyc_event_id,
  get_json_object(resp_raw_data, '$.data.kyc_status_raw') AS kyc_status_raw,
  -- 使用 get_json_object 根据 field 值获取对应的 score
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="pob")].score') AS DOUBLE) AS score_pob,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="full_name")].score') AS DOUBLE) AS score_full_name,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="address")].score') AS DOUBLE) AS score_address,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="province")].score') AS DOUBLE) AS score_province,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="city")].score') AS DOUBLE) AS score_city,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="dob")].score') AS DOUBLE) AS score_dob,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="district")].score') AS DOUBLE) AS score_district,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="village")].score') AS DOUBLE) AS score_village,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="nik")].score') AS DOUBLE) AS score_nik,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="mother_maiden_name")].score') AS DOUBLE) AS score_mother_maiden_name,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="family_card_no")].score') AS DOUBLE) AS score_family_card_no,
  -- 计算所有 score 的平均值
  (
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="pob")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="full_name")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="address")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="province")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="city")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="dob")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="district")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="village")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="nik")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="mother_maiden_name")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="family_card_no")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="selfiePhoto")].score') AS DOUBLE) +
    CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="liveness")].score') AS DOUBLE)
  ) / 13 AS score_demographic,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="selfiePhoto")].score') AS DOUBLE) AS score_selfiephoto,
  CAST(get_json_object(resp_raw_data, '$.data.fields[?(@.field=="liveness")].score') AS DOUBLE) AS score_liveness,
  trace_id AS trace_id,
  create_time AS create_time,
  update_time AS update_time,
  current_date() AS grass_date
FROM risk.ods__credit_risk_datafetch_id_db__identity_data_tab__df
WHERE data_source_name = 'VidaData'
;
```

主要修改：
1. 使用 JSONPath 表达式 `[?(@.field=="xxx")]` 来根据 `field` 的值查找对应的对象
2. 然后从找到的对象中获取 `score` 值
3. 所有字段都使用这种方式，不再依赖数组索引
4. 计算 `score_demographic` 时也使用相同的方式获取每个字段的 score

这样修改后，不管 fields 数组中的对象顺序如何，都能正确获取到对应的 score 值。

---

_**User**_

用  [?(@.field=="xxx")] 这种方法找对应 field = xxx 在一个 json 体内的另一个key ’score‘的对应 cvalue 的方法你有没有其他的方式呢？更准确且稳定的方式，或者你可以给我解释下 这种 处理方式的原理和逻辑是什么吗？

---

_**Assistant**_



---

在Spark SQL中解析JSON数组并查找特定字段值有几种方法，我来详细解释一下：

### 1. 关于 `[?(@.field=="xxx")]` 的原理

这是JSONPath表达式，其中：
- `[]` 表示数组
- `?()` 表示过滤条件
- `@` 表示当前元素
- `@.field=="xxx"` 表示当前元素的field属性等于"xxx"

但实际上，Spark的`get_json_object`函数**不完全**支持这种高级JSONPath查询，它只支持基本的`.`和`[]`操作，不支持过滤表达式。

### 2. 更可靠的替代方案

#### 方案一：使用JSON_QUERY或json_array_elements（适用于较新版本Spark）

如果你使用的是较新版本的Spark，可以使用以下方法：

```sql
SELECT 
  get_json_object(
    element_at(
      filter(
        from_json(
          get_json_object(resp_raw_data, '$.data.fields'), 
          'array<struct<field:string,score:double>>'
        ),
        x -> x.field = 'pob'
      ), 
      1
    ).score
  ) AS score_pob
FROM table;
```

#### 方案二：使用自定义UDF函数

```sql
-- 注册UDF
CREATE OR REPLACE FUNCTION get_score_by_field(json_array STRING, field_name STRING)
RETURNS DOUBLE
LANGUAGE SCALA
AS $$
  import org.json4s._
  import org.json4s.jackson.JsonMethods._
  
  try {
    val jsonArray = parse(json_array).asInstanceOf[JArray]
    val fieldObj = jsonArray.arr.find(json => 
      (json \ "field").extract[String] == field_name
    )
    fieldObj.map(json => (json \ "score").extract[Double]).getOrElse(0.0)
  } catch {
    case _: Throwable => 0.0
  }
$$;

-- 使用UDF
SELECT 
  get_score_by_field(get_json_object(resp_raw_data, '$.data.fields'), 'pob') AS score_pob
FROM table;
```

#### 方案三：使用LATERAL VIEW（最通用的方法）

```sql
WITH exploded_fields AS (
  SELECT 
    resp_raw_data,
    header_data,
    request_key,
    field_obj.field AS field_name,
    field_obj.score AS field_score
  FROM table
  LATERAL VIEW EXPLODE(
    FROM_JSON(
      get_json_object(resp_raw_data, '$.data.fields'),
      'array<struct<field:string,score:double>>'
    )
  ) fields AS field_obj
)

SELECT
  resp_raw_data,
  MAX(CASE WHEN field_name = 'pob' THEN field_score END) AS score_pob,
  MAX(CASE WHEN field_name = 'full_name' THEN field_score END) AS score_full_name
FROM exploded_fields
GROUP BY resp_raw_data;
```

这第三种方法是最稳定的，因为它：
1. 先将JSON数组展开成多行
2. 然后用标准SQL的CASE WHEN聚合出各字段值
3. 不依赖JSONPath的高级特性，更具兼容性
4. 对数组中元素顺序完全不敏感

在实际生产环境中，我建议使用方案三，因为它对Spark版本要求低，逻辑清晰，且最不容易出错。

---

_**User**_

你使用 方案三 帮我改一下 sql，然后应用到sql script 上面去 

---

_**Assistant**_



---

